{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manipular grandes volúmenes de datos utilizando Spark SQL para\n",
    "# resolver un problema\n",
    "\n",
    "# Machine learning para clasificar\n",
    "# Python es el lenguaje preferido para la ciencia de datos debido a NumPy,\n",
    "# Pandas y matplotlib, que son herramientas que facilitan el trabajo con\n",
    "# matrices y dibujan gráficos y pueden trabajar con grandes matrices de datos\n",
    "# de manera eficiente. Pero Spark está diseñado para trabajar con una enorme\n",
    "# cantidad de datos, distribuidos en un clúster.\n",
    "\n",
    "# Datos del paciente del corazón\n",
    "# Encontrará adjunto la base de datos.\n",
    "# Las columnas son:\n",
    "# 1. Años\n",
    "# 2. Sexo\n",
    "# 3. Tipo de dolor torácico (4 valores)\n",
    "# 4. Presión arterial en reposo\n",
    "# 5. Colesterol sérico en mg/dl\n",
    "# 6. Azúcar en sangre en ayunas > 120 mg/dl\n",
    "# 7. Resultados electrocardiográficos en reposo (valores 0,1,2)\n",
    "# 8. Frecuencia cardíaca máxima alcanzada\n",
    "# 9. Angina inducida por el ejercicio\n",
    "# 10. Oldpeak = depresión del ST inducida por el ejercicio en relación con el reposo\n",
    "# 11. Pendiente del segmento ST de ejercicio máximo\n",
    "# 12. Número de vasos principales (0-3) coloreados por fluoroscopia\n",
    "# 13. Thal: 3 = normal; 6 = defecto fijo; 7 = defecto reversible\n",
    "\n",
    "# El campo que indica si el paciente tiene un problema cardíaco. Los números\n",
    "# son los siguientes:\n",
    "\n",
    "# Un valor de 3 significa que el paciente está sano (normal). Un valor de 6\n",
    "# significa que se ha solucionado el problema de salud del paciente. Un valor de\n",
    "# 7 significa que se puede solucionar el problema de salud del paciente.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Años: double (nullable = true)\n",
      " |-- Sexo: double (nullable = true)\n",
      " |-- Tipo_dolor_toracico: double (nullable = true)\n",
      " |-- Presion_arterial_reposo: double (nullable = true)\n",
      " |-- Colesterol_serico: double (nullable = true)\n",
      " |-- Azucar_sangre_ayunas: double (nullable = true)\n",
      " |-- Resultados_electrocardiográficos_reposo: double (nullable = true)\n",
      " |-- Frecuencia_cardíaca_máxima: double (nullable = true)\n",
      " |-- Angina_inducida_ejercicio: double (nullable = true)\n",
      " |-- Oldpeak: double (nullable = true)\n",
      " |-- Pendiente_segmento ST: double (nullable = true)\n",
      " |-- Número_vasos_principales: double (nullable = true)\n",
      " |-- Thal: double (nullable = true)\n",
      "\n",
      "+----+----+-------------------+-----------------------+-----------------+--------------------+---------------------------------------+--------------------------+-------------------------+-------+---------------------+------------------------+----+\n",
      "|Años|Sexo|Tipo_dolor_toracico|Presion_arterial_reposo|Colesterol_serico|Azucar_sangre_ayunas|Resultados_electrocardiográficos_reposo|Frecuencia_cardíaca_máxima|Angina_inducida_ejercicio|Oldpeak|Pendiente_segmento ST|Número_vasos_principales|Thal|\n",
      "+----+----+-------------------+-----------------------+-----------------+--------------------+---------------------------------------+--------------------------+-------------------------+-------+---------------------+------------------------+----+\n",
      "|70.0| 1.0|                4.0|                  130.0|            322.0|                 0.0|                                    2.0|                     109.0|                      0.0|    2.4|                  2.0|                     3.0| 3.0|\n",
      "|67.0| 0.0|                3.0|                  115.0|            564.0|                 0.0|                                    2.0|                     160.0|                      0.0|    1.6|                  2.0|                     0.0| 7.0|\n",
      "|57.0| 1.0|                2.0|                  124.0|            261.0|                 0.0|                                    0.0|                     141.0|                      0.0|    0.3|                  1.0|                     0.0| 7.0|\n",
      "|64.0| 1.0|                4.0|                  128.0|            263.0|                 0.0|                                    0.0|                     105.0|                      1.0|    0.2|                  2.0|                     1.0| 7.0|\n",
      "|74.0| 0.0|                2.0|                  120.0|            269.0|                 0.0|                                    2.0|                     121.0|                      1.0|    0.2|                  1.0|                     1.0| 3.0|\n",
      "|65.0| 1.0|                4.0|                  120.0|            177.0|                 0.0|                                    0.0|                     140.0|                      0.0|    0.4|                  1.0|                     0.0| 7.0|\n",
      "|56.0| 1.0|                3.0|                  130.0|            256.0|                 1.0|                                    2.0|                     142.0|                      1.0|    0.6|                  2.0|                     1.0| 6.0|\n",
      "|59.0| 1.0|                4.0|                  110.0|            239.0|                 0.0|                                    2.0|                     142.0|                      1.0|    1.2|                  2.0|                     1.0| 7.0|\n",
      "|60.0| 1.0|                4.0|                  140.0|            293.0|                 0.0|                                    2.0|                     170.0|                      0.0|    1.2|                  2.0|                     2.0| 7.0|\n",
      "|63.0| 0.0|                4.0|                  150.0|            407.0|                 0.0|                                    2.0|                     154.0|                      0.0|    4.0|                  2.0|                     3.0| 7.0|\n",
      "|59.0| 1.0|                4.0|                  135.0|            234.0|                 0.0|                                    0.0|                     161.0|                      0.0|    0.5|                  2.0|                     0.0| 7.0|\n",
      "|53.0| 1.0|                4.0|                  142.0|            226.0|                 0.0|                                    2.0|                     111.0|                      1.0|    0.0|                  1.0|                     0.0| 7.0|\n",
      "|44.0| 1.0|                3.0|                  140.0|            235.0|                 0.0|                                    2.0|                     180.0|                      0.0|    0.0|                  1.0|                     0.0| 3.0|\n",
      "|61.0| 1.0|                1.0|                  134.0|            234.0|                 0.0|                                    0.0|                     145.0|                      0.0|    2.6|                  2.0|                     2.0| 3.0|\n",
      "|57.0| 0.0|                4.0|                  128.0|            303.0|                 0.0|                                    2.0|                     159.0|                      0.0|    0.0|                  1.0|                     1.0| 3.0|\n",
      "|71.0| 0.0|                4.0|                  112.0|            149.0|                 0.0|                                    0.0|                     125.0|                      0.0|    1.6|                  2.0|                     0.0| 3.0|\n",
      "|46.0| 1.0|                4.0|                  140.0|            311.0|                 0.0|                                    0.0|                     120.0|                      1.0|    1.8|                  2.0|                     2.0| 7.0|\n",
      "|53.0| 1.0|                4.0|                  140.0|            203.0|                 1.0|                                    2.0|                     155.0|                      1.0|    3.1|                  3.0|                     0.0| 7.0|\n",
      "|64.0| 1.0|                1.0|                  110.0|            211.0|                 0.0|                                    2.0|                     144.0|                      1.0|    1.8|                  2.0|                     0.0| 3.0|\n",
      "|40.0| 1.0|                1.0|                  140.0|            199.0|                 0.0|                                    0.0|                     178.0|                      1.0|    1.4|                  1.0|                     0.0| 7.0|\n",
      "+----+----+-------------------+-----------------------+-----------------+--------------------+---------------------------------------+--------------------------+-------------------------+-------+---------------------+------------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# importado de librerias\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# Crear una configuración de Spark\n",
    "conf = SparkConf().setAppName(\"MyApp\").setMaster(\"local[*]\")\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "\n",
    "# Cargar el dataset con esta definicion de columnas y tipos\n",
    "\n",
    "df = spark.read.csv(\"heart.dat\", header=False, sep=' ', inferSchema=True)\n",
    "\n",
    "# set column names\n",
    "schema = ['Años','Sexo','Tipo_dolor_toracico','Presion_arterial_reposo','Colesterol_serico','Azucar_sangre_ayunas',\n",
    "            'Resultados_electrocardiográficos_reposo', 'Frecuencia_cardíaca_máxima', 'Angina_inducida_ejercicio',\n",
    "            'Oldpeak', 'Pendiente_segmento ST','Número_vasos_principales','Thal','Target']\n",
    "for i in range(len(schema)):\n",
    "    col = '_c'+str(i) # nombre de la columna\n",
    "    #df = df.withColumn(col, df[col].cast(FloatType())) # convertir a FloatType\n",
    "    df = df.withColumnRenamed(col, schema[i]) # renombrar columnas\n",
    "    \n",
    "# drop column Target\n",
    "df = df.drop('Target')\n",
    "\n",
    "   \n",
    "\n",
    "# Ver el esquema de los datos\n",
    "df.printSchema()\n",
    "\n",
    "# Ver los datos\n",
    "df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-------------------+-----------------------+-----------------+--------------------+---------------------------------------+--------------------------+-------------------------+-------+---------------------+------------------------+----+------+\n",
      "|Años|Sexo|Tipo_dolor_toracico|Presion_arterial_reposo|Colesterol_serico|Azucar_sangre_ayunas|Resultados_electrocardiográficos_reposo|Frecuencia_cardíaca_máxima|Angina_inducida_ejercicio|Oldpeak|Pendiente_segmento ST|Número_vasos_principales|Thal|Target|\n",
      "+----+----+-------------------+-----------------------+-----------------+--------------------+---------------------------------------+--------------------------+-------------------------+-------+---------------------+------------------------+----+------+\n",
      "|70.0| 1.0|                4.0|                  130.0|            322.0|                 0.0|                                    2.0|                     109.0|                      0.0|    2.4|                  2.0|                     3.0| 3.0|     0|\n",
      "|67.0| 0.0|                3.0|                  115.0|            564.0|                 0.0|                                    2.0|                     160.0|                      0.0|    1.6|                  2.0|                     0.0| 7.0|     1|\n",
      "|57.0| 1.0|                2.0|                  124.0|            261.0|                 0.0|                                    0.0|                     141.0|                      0.0|    0.3|                  1.0|                     0.0| 7.0|     1|\n",
      "|64.0| 1.0|                4.0|                  128.0|            263.0|                 0.0|                                    0.0|                     105.0|                      1.0|    0.2|                  2.0|                     1.0| 7.0|     1|\n",
      "|74.0| 0.0|                2.0|                  120.0|            269.0|                 0.0|                                    2.0|                     121.0|                      1.0|    0.2|                  1.0|                     1.0| 3.0|     0|\n",
      "+----+----+-------------------+-----------------------+-----------------+--------------------+---------------------------------------+--------------------------+-------------------------+-------+---------------------+------------------------+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Entonces, escriba esta función ENFERMO() para marcar 0 como negativo y 1\n",
    "# como positivo, porque la regresión logística binaria requiere uno de dos\n",
    "# resultados.\n",
    "\n",
    "# Importar librerias\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "# Crear la funcion\n",
    "def ENFERMO(x):\n",
    "    if x in(3, 6): # 3 = normal; 6 = problema solucionado\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "# Crear la funcion UDf\n",
    "udfENFERMO = udf(ENFERMO, IntegerType())\n",
    "\n",
    "# Aplicar la funcion UDF a la columna target usando el valor de that\n",
    "df = df.withColumn(\"Target\", udfENFERMO(\"Thal\") )\n",
    "\n",
    "# Ver los datos\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|            features|Target|\n",
      "+--------------------+------+\n",
      "|[70.0,1.0,4.0,130...|     0|\n",
      "|[67.0,0.0,3.0,115...|     1|\n",
      "|[57.0,1.0,2.0,124...|     1|\n",
      "|[64.0,1.0,4.0,128...|     1|\n",
      "|[74.0,0.0,2.0,120...|     0|\n",
      "+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# También debe crear el dataframe de Spark raw_data usando la operación\n",
    "# transform() y seleccionando solo la columna de características.\n",
    "\n",
    "# Importar librerias\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# set columns except the col Target\n",
    "columns = df.columns\n",
    "columns.remove('Target')\n",
    "\n",
    "\n",
    "# Crear el vector assembler\n",
    "assembler = VectorAssembler(inputCols=columns, outputCol='features')\n",
    "\n",
    "# Transformar los datos\n",
    "raw_data = assembler.transform(df).select(\"features\", \"Target\")\n",
    "\n",
    "# Ver los datos\n",
    "raw_data.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Use Standard Scaler para poner todos los números en la misma escala.\n",
    "# Esto toma la observación y resta la media, y luego la divide por la\n",
    "# desviación estándar.\n",
    "\n",
    "# Importar librerias\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "# Crear el objeto StandardScaler\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",\n",
    "                        withStd=True, withMean=True)\n",
    "\n",
    "# Calcular la media y la desviacion estandar\n",
    "scalerModel = scaler.fit(raw_data) \n",
    "\n",
    "# Normalizar los datos\n",
    "scaledData = scalerModel.transform(raw_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+--------------------+\n",
      "|            features|Target|      scaledFeatures|\n",
      "+--------------------+------+--------------------+\n",
      "|[70.0,1.0,4.0,130...|     0|[1.70892007713705...|\n",
      "|[67.0,0.0,3.0,115...|     1|[1.37957787811706...|\n",
      "|[57.0,1.0,2.0,124...|     1|[0.28177054805043...|\n",
      "|[64.0,1.0,4.0,128...|     1|[1.05023567909707...|\n",
      "|[74.0,0.0,2.0,120...|     0|[2.14804300916370...|\n",
      "+--------------------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. Imprima el estado actual de sus datos\n",
    "\n",
    "# Ver los datos\n",
    "scaledData.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+--------------------+\n",
      "|            features|Target|      scaledFeatures|\n",
      "+--------------------+------+--------------------+\n",
      "|(13,[0,2,3,4,7,10...|     0|[-1.9138441120828...|\n",
      "|(13,[0,2,3,4,7,10...|     0|[-1.4747211800561...|\n",
      "|(13,[0,2,3,4,7,10...|     0|[-1.3649404470495...|\n",
      "|(13,[0,2,3,4,7,10...|     0|[-1.0355982480295...|\n",
      "|(13,[0,2,3,4,7,10...|     0|[-0.9258175150228...|\n",
      "+--------------------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. Divida los datos en conjuntos de datos de entrenamiento y prueba. Use 50/50.\n",
    "train_data, test_data = scaledData.randomSplit([0.5, 0.5])\n",
    "\n",
    "train_data.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coeficientes: [-3.4951626823105677,-26.679313464084693,-4.4087383198706105,-0.6424689176541718,9.237879168078045,-1.5592462686678907,-4.8878516394218,7.978132017061151,4.2115804052136,6.492090146151653,-6.673431922172488,-1.1174446075894764,75.03453148392056]\n",
      "Intercepto: -37.573629977042906\n"
     ]
    }
   ],
   "source": [
    "# 4. Cree un modelo de regresión logística y entrénalo.\n",
    "\n",
    "# Importar librerias\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Crear el objeto LogisticRegression\n",
    "model = LogisticRegression(labelCol=\"Target\", featuresCol=\"scaledFeatures\")\n",
    "\n",
    "# Entrenar el modelo\n",
    "trainedModel = model.fit(train_data)\n",
    "\n",
    "# Imprima los coeficientes y el intercepto para el modelo de regresión logística\n",
    "print(\"Coeficientes: \" + str(trainedModel.coefficients))\n",
    "print(\"Intercepto: \" + str(trainedModel.intercept))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+--------------------+--------------------+--------------------+----------+-------+\n",
      "|            features|Target|      scaledFeatures|       rawPrediction|         probability|prediction|correct|\n",
      "+--------------------+------+--------------------+--------------------+--------------------+----------+-------+\n",
      "|(13,[0,2,3,4,7,10...|     0|[-1.6942826460694...|[51.5153470684896...|           [1.0,0.0]|       0.0|      1|\n",
      "|(13,[0,2,3,4,7,10...|     0|[0.39155128105709...|[36.9507097314047...|           [1.0,0.0]|       0.0|      1|\n",
      "|(13,[0,2,3,4,7,10...|     0|[0.83067421308374...|[69.7829005123032...|           [1.0,0.0]|       0.0|      1|\n",
      "|[29.0,1.0,2.0,130...|     0|[-2.7920899761361...|[102.297827209384...|           [1.0,0.0]|       0.0|      1|\n",
      "|[34.0,0.0,2.0,118...|     0|[-2.2431863111028...|[35.4083777080133...|[0.99999999999999...|       0.0|      1|\n",
      "+--------------------+------+--------------------+--------------------+--------------------+----------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Aciertos:  143  de  144\n"
     ]
    }
   ],
   "source": [
    "# 5. Finalmente Use la función F de Spark SQL para crear una nueva columna\n",
    "# correcta cuando ENFERMO() es igual a la predicción, lo que significa que\n",
    "# el resultado predicho es igual a los resultados reales.\n",
    "\n",
    "# Importar librerias\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Crear la funcion\n",
    "def CHECK_RESULT(x, y):\n",
    "    if int(x) == int(y):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Crear la funcion UDf\n",
    "udfCHECK_RESULT = udf(CHECK_RESULT, IntegerType())\n",
    "\n",
    "# Crear la columna correcta\n",
    "predictions = trainedModel.transform(test_data)\n",
    "predictions = predictions.withColumn(\"correct\", udfCHECK_RESULT(\"Target\", \"prediction\"))\n",
    "\n",
    "# Ver los datos\n",
    "predictions.show(5)\n",
    "\n",
    "# conteo de datos\n",
    "print(f\"Aciertos: \",predictions.filter(predictions.correct == 1).count(),\" de \", predictions.count())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cerrar la sesion de spark\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - SQL Spark\n",
    "\n",
    "# SQL Spark es un módulo de Spark para el procesamiento de datos estructurados.\n",
    "# Proporciona una API para manipular datos estructurados utilizando el lenguaje\n",
    "# SQL estándar. Puede ejecutar consultas SQL escritas en el lenguaje SQL estándar\n",
    "# utilizando Spark SQL. Spark SQL proporciona una API para manipular datos\n",
    "# estructurados utilizando el lenguaje SQL estándar. Puede ejecutar consultas SQL\n",
    "# escritas en el lenguaje SQL estándar utilizando Spark SQL. Spark SQL proporciona\n",
    "# una API para manipular datos estructurados utilizando el lenguaje SQL estándar.\n",
    "# Puede ejecutar consultas SQL escritas en el lenguaje SQL estándar utilizando\n",
    "# Spark SQL.\n",
    "\n",
    "# - Formatos de Archivo\n",
    "# Spark SQL proporciona una API para leer archivos de texto, JSON, CSV, parquet,\n",
    "# ORC y muchos otros formatos de archivo. Puede leer datos de estos formatos y\n",
    "# crear un DataFrame a partir de ellos. También puede escribir datos en estos\n",
    "# formatos.\n",
    "\n",
    "\n",
    "# - Machine Learning con Spark\n",
    "# Spark proporciona una API para el aprendizaje automático llamada MLlib. Puede\n",
    "# usar MLlib para crear modelos de aprendizaje automático. MLlib proporciona\n",
    "# algoritmos de aprendizaje automático como clasificación, regresión, agrupación,\n",
    "# filtrado colaborativo, reducción de dimensionalidad, así como tuberías de\n",
    "# aprendizaje automático y extracción de características. MLlib también admite\n",
    "# herramientas de terceros como TensorFlow. Puede usar TensorFlow para crear\n",
    "# modelos de aprendizaje automático y usar MLlib para entrenarlos y usarlos para\n",
    "# predicciones.\n",
    "\n",
    "# - MLlib\n",
    "# MLlib es la biblioteca de aprendizaje automático de Spark. Proporciona\n",
    "# algoritmos de aprendizaje automático como clasificación, regresión, agrupación,\n",
    "# filtrado colaborativo, reducción de dimensionalidad, así como tuberías de\n",
    "# aprendizaje automático y extracción de características. MLlib también admite\n",
    "# herramientas de terceros como TensorFlow. Puede usar TensorFlow para crear\n",
    "# modelos de aprendizaje automático y usar MLlib para entrenarlos y usarlos para\n",
    "# predicciones.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
